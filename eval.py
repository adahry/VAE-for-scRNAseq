# -*- coding: utf-8 -*-
"""SAD2022Z_Report_Ada_Hryniewicka.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1blR5guzwJOAeTN10GCYJTXX778tn0-Lj

"""

'''
!pip install import_ipynb
!pip install ipynb
from google.colab import drive
drive.mount('/content/drive/')
import import_ipynb
import ipynb
PATH_TRAIN='/content/drive/MyDrive/sad2_project_data/SAD2022Z_Project1_GEX_train.h5ad'
PATH_TEST='/content/drive/MyDrive/sad2_project_data/SAD2022Z_Project1_GEX_test.h5ad'
'''
import os

#ypur models will be saved in models folder in your working directory
if not os.path.exists("images"):
    # if the demo_folder directory is not present 
    # then create it.
    os.mkdir("images")


import torch
from torch.optim import Adam
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
import seaborn as sns


from autoencoder import Encoder, Decoder, VAE, loss_function
from data_loading import get_data, normalize

import argparse
from pathlib import Path

parser = argparse.ArgumentParser()

parser.add_argument("path_train")
parser.add_argument("path_test")

args= parser.parse_args()

PATH_TRAIN = Path(args.path_train)
PATH_TEST = Path(args.path_test)

if not PATH_TRAIN.exists():
    print("The test directory doesn't exist")
    raise SystemExit(1)

if not PATH_TEST.exists():
    print("The train directory doesn't exist")
    raise SystemExit(1)



adata_train, adata_test, data_raw, data_X,data_test_raw, data_test_X = get_data(PATH_TRAIN, PATH_TEST)

"""# 1. Exploration"""

print("Observations and variables numbers for train dataset:",adata_train.n_obs, adata_test.n_vars)
print("Observations and variables numbers for test dataset:",adata_test.n_obs, adata_test.n_vars)

"""Histograms showing the data distribution."""

hist, bins_edges = np.histogram(data_raw, bins=200)
hist2, bins_edges2 = np.histogram(data_X, bins=200)

plt.figure(figsize=(15,10))
plt.subplot(221)
plt.hist(bins_edges[:-1], bins_edges, weights=hist,alpha=0.5 , color='orange')
plt.title('Raw full')
plt.xlim(0,2000)
plt.subplot(222)
plt.hist(bins_edges2[:-1], bins_edges2, weights=hist2,alpha=0.5, color='blue')
plt.title('Preprocessed full')
plt.xlim(0,2000000)
plt.subplot(223)
plt.hist(bins_edges[:-2], bins_edges[1:], weights=hist[1:], alpha=0.5, color='orange')
plt.xlim(0,20000)
plt.title('Raw without 0')
plt.subplot(224)
plt.hist(bins_edges2[:-2], bins_edges2[1:], weights=hist2[1:], alpha=0.5, color='blue')
plt.xlim(0,2000000)
plt.title('Preprocessed without 0')
plt.savefig('images/hist.png')
plt.show()

"""The abudance of zeros overwhelme the plots. The second histograms are showing data excluding zeros.

Researchers view vast zeros in single-cell RNA-seq data differently: some regard zeros as biological signals representing no or low gene expression, while others regard zeros as missing data to be corrected.

Firstly, Poisson distribution comes to mind because it is for discrete data but here the decision was to choose exponential distribution which is a kind of a generalization of Poisson for coninous data. It can be helpful when doing operations like normalization and work with no-integers also. What is more, later when there is modification of decoder according to distribution there is no sampling from it for loss function (using probability) so choosing continous distribution instead of discrete should not have negative effect.
"""

print("Means", "\nRaw:", np.mean(hist), "\nCleaned:",np.mean(hist2))
print("\nMedian", "\nRaw:", np.median(hist), "\nCleaned:",np.median(hist2))
print("\nMax", "\nRaw:", max(hist), "\nCleaned:",max(hist2))
print("\nMin", "\nRaw:", min(hist), "\nCleaned:",min(hist2))

"""Cleaned dataset (adata.X) was prepared by dividing raw dataset counts(adata.layers["counts"]) by GEX_size_factor for each cell- normalization. The log1p transformation data is stored in adata.layers["log_norm"]. 
 

"""

idx=20
print('For index:', idx)
print("preprocessed:",data_X[1][idx])
print("raw:",data_raw[1][idx])
print('raw after dividing by GEX_size_facore:',(data_raw[1][idx])/adata_train.obs['GEX_size_factors'][idx])

"""The adata_train.obs contains information about detailed information about cells. They are shown below."""

adata_train.obs.columns

print("Number of patients:", len(adata_train.obs["DonorID"].unique()))
print("Number of cell types:", len(adata_train.obs["cell_type"].unique()))
print("Number of labs:", len(adata_train.obs["Site"].unique()))

"""# 2. Vanilla VAE training"""

def read_model(PATH,latent_dim):  
  x_dim = list(data_raw.shape)[1] 
  hidden_dim = 400
  lr = 1e-5

  encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
  decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

  model = VAE(Encoder=encoder, Decoder=decoder)
  optimizer = Adam(model.parameters(), lr=lr)


  checkpoint = torch.load(PATH)
  model.load_state_dict(checkpoint['model_state_dict'])
  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
  epoch = checkpoint['epoch']
  loss = checkpoint['loss']
  loss_val = checkpoint['loss_val']
  reclos = checkpoint['reclos_array']
  reclos_val = checkpoint['reclos_val_array']
  kld = checkpoint['kldlos_array']  
  kld_val = checkpoint['kldlos_val_array']

  return loss,loss_val,reclos, reclos_val, kld, kld_val, epoch

def ploting_trained(epochs_array,loss_array,loss_val_array, title,loss_name):
  plt.plot(epochs_array, loss_array, '-', label='train')
  plt.plot(epochs_array, loss_val_array, '-', label='test')
  plt.xlabel('epoch')
  plt.ylabel(loss_name)
  plt.legend()
  plt.title(title)

"""The learning curves for Vanila VAE and latent space- 100."""

loss,loss_val2,reclos, reclos_val, kld, kld_val, epoch = read_model(PATH='models/model_normal50.pt',latent_dim=50)

loss,loss_val3,reclos, reclos_val, kld, kld_val, epoch = read_model(PATH='models/model_normal300.pt',latent_dim=300)

loss,loss_val1,reclos, reclos_val, kld, kld_val, epoch = read_model(PATH='models/model_normal100.pt', latent_dim=100)

"""Comparing the -ELBO losses for different latent spaces."""

d = {'Latent space 100': loss_val1[-1], 'Latent space 50': loss_val2[-1], 'Latent space 300': loss_val3[-1]}
print(pd.DataFrame(data=d, index=['-ELBO']))

plt.figure(figsize=(20,5))
plt.subplot(131)
ploting_trained(epoch,loss,loss_val1,'Size of latent space: 100','-ELBO')
plt.subplot(132)
ploting_trained(epoch,reclos,reclos_val,'Size of latent space: 100','Regularization losses')
plt.subplot(133)
ploting_trained(epoch,kld,kld_val,'Size of latent space: 100','KLD')
plt.savefig('images/learning_normal_curves.png')
plt.show()

"""The lowest -ELBO is for latent space 100 and this size was chosen for further computations."""

x_dim = list(data_raw.shape)[1] 
hidden_dim = 400
lr = 1e-5
latent_dim=100
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=lr)


checkpoint = torch.load('models/model_normal100.pt')
model.load_state_dict(checkpoint['model_state_dict'])

normdat=normalize(data_test_raw)
z,_,_= model.Encoder(normdat)
pca = PCA(.95)
principalComponents = pca.fit_transform(z.detach().numpy())
principalDf = pd.DataFrame(data = principalComponents)
print('The number of principal components that explains more than 95% of the variance: {}'.format(len(pca.explained_variance_ratio_)))

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(z.detach().numpy())
principalDf = pd.DataFrame(data = principalComponents,columns = ['PC1', 'PC2'])
principalDf['cell_type']=adata_test.obs.cell_type.values
plt.figure(figsize=(26,18))
sns.scatterplot(
    x="PC1", y="PC2",
    hue="cell_type",
    palette=sns.color_palette("hls", len(adata_test.obs.cell_type.values.unique())),
    data=principalDf,
    legend="full",
    alpha=0.3
)

plt.savefig('images/pca_normal.png')
plt.show()

"""The Vanila VAE does not provide reasonable restults so it needs to be improven by changing the distribution in Custom Decoder. The exponential disribution was chosen. The same results were on different labels coloring. This decoder has not done proper job.

# 3. Custom Decoder
"""


from vae_newdecoder import Encoder,Decoder,VAE, loss_function


loss,loss_val1,reclos, reclos_val, kld, kld_val, epoch = read_model(PATH='models/model_exp100.pt',latent_dim=100)
loss,loss_val3,reclos, reclos_val, kld, kld_val, epoch = read_model(PATH='models/model_exp300.pt',latent_dim=300)
loss,loss_val2,reclos, reclos_val, kld, kld_val, epoch = read_model(PATH='models/model_exp50.pt',latent_dim=50)

d = {'Latent space 100': loss_val1[-1], 'Latent space 50': loss_val2[-1], 'Latent space 300': loss_val3[-1]}
print(pd.DataFrame(data=d, index=['-ELBO']))


plt.figure(figsize=(20,5))
plt.subplot(131)
ploting_trained(epoch[1:],loss[1:],loss_val2[1:],' ','-ELBO')
plt.subplot(132)
ploting_trained(epoch[1:],reclos[1:],reclos_val[1:],'Learning curves for custom decoder','Regularization losses')
plt.subplot(133)
ploting_trained(epoch[1:],kld[1:],kld_val[1:],' ','KLD')
plt.savefig('images/learining_exponential_curves.png')
plt.show()

latent_dim=50
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=lr)


checkpoint = torch.load('models/model_exp50.pt')
model.load_state_dict(checkpoint['model_state_dict'])

testdata=torch.tensor(data_test_raw.values,dtype=torch.float32)
z,_,_= model.Encoder(testdata)
pca = PCA(.95)
principalComponents = pca.fit_transform(z.detach().numpy())
principalDf = pd.DataFrame(data = principalComponents)
print('The number of principal components that explains more than 95% of the variance for new decoder: {}'.format(len(pca.explained_variance_ratio_)))


pca = PCA(n_components=2)
principalComponents = pca.fit_transform(z.detach().numpy())
principalDf = pd.DataFrame(data = principalComponents,columns = ['PC1', 'PC2'])
principalDf['cell_type']=adata_test.obs.cell_type.values
plt.figure(figsize=(20,15))
sns.scatterplot(
    x="PC1", y="PC2",
    hue="cell_type",
    palette=sns.color_palette("hls", len(adata_test.obs.cell_type.values.unique())),
    data=principalDf,
    legend="full",
    alpha=0.3,
)
plt.title('CELL TYPE')
plt.savefig('images/pca_exp_cell.png')
plt.show()

principalDf['batch']=adata_test.obs.batch.values
plt.figure(figsize=(20,15))
sns.scatterplot(
    x="PC1", y="PC2",
    hue="batch",
    palette=sns.color_palette("hls", len(adata_test.obs.batch.values.unique())),
    data=principalDf,
    legend="full",
    alpha=0.3
)
plt.title('BATCH')
plt.savefig('images/pca_exp_batch.png')
plt.show()

p=pd.Categorical(adata_test.obs.DonorID.values.astype(str))
principalDf['DonorID']=p
plt.figure(figsize=(20,15))
sns.scatterplot(
    x="PC1", y="PC2",
    hue="DonorID",
    palette=sns.color_palette("hls", len(p.unique())),
    data=principalDf,
    legend="full",
    alpha=0.3
)
plt.title('DONOR ID')
plt.savefig('images/pca_exp_donor.png')
plt.show()

principalDf['site']=adata_test.obs.Site.values
plt.figure(figsize=(20,15))
sns.scatterplot(
    x="PC1", y="PC2",
    hue="site",
    palette=sns.color_palette("hls", len(adata_test.obs.Site.values.unique())),
    data=principalDf,
    legend="full",
    alpha=0.3
)
plt.title('LABORATORY')
plt.savefig('images/pca_exp_lab.png')
plt.show()

