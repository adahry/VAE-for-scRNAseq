# -*- coding: utf-8 -*-
"""AUTOENCODER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_pkU0Sg6w7zBWzY60GxwdytuaMjr7es
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal,MultivariateNormal  


class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim):
      super(Encoder, self).__init__()

      self.FC_input = nn.Linear(input_dim, hidden_dim)
      self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)
      self.FC_mean  = nn.Linear(hidden_dim, latent_dim)
      self.FC_var   = nn.Linear (hidden_dim, latent_dim)
      
      self.ReLU = nn.ReLU()
      
      self.training = True

   
  def forward(self, x):
      h = self.ReLU(self.FC_input(x))
      h = self.ReLU(self.FC_input2(h))
      mean = self.FC_mean(h)
      log_var = self.ReLU(self.FC_var(h))  
      sd = torch.exp(0.5*log_var)   
      eps=torch.randn_like(sd) #samplowanie z rozkladu normalnego 
      z=mean+eps*sd             
      return z, mean, log_var

class Decoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super(Decoder, self).__init__()
        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)
        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)

        self.FC_mean  = nn.Linear(hidden_dim, output_dim)
        
        self.ReLU = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.training = True
    def forward(self, x):
        h     = self.ReLU(self.FC_hidden(x))
        h     = self.ReLU(self.FC_hidden2(h))   
        #x_hat = torch.sigmoid(self.FC_output(h)) #to jesli chcemy miec output pomiedzy 0 i 1
        #h = self.FC_output(h)
        mean = self.FC_mean(h)
        sd = torch.tensor([0.1])
        dist=Normal(mean, sd)
        sample = dist.rsample()
        
        x_hat=self.sigmoid(sample)

        return x_hat,mean,sd

class VAE(nn.Module):
    def __init__(self, Encoder, Decoder):
        super(VAE, self).__init__()
        self.Encoder = Encoder
        self.Decoder = Decoder
                       
    def forward(self, x):
        z, mean, log_var = self.Encoder(x)
        x_hat,mean_d,sd_d = self.Decoder(z)
        return x_hat, mean, log_var, mean_d, sd_d

def loss_function(x, x_hat,mean,log_var,mean_d, sd_d):
    #MSE = F.mse_loss(x_hat, x, reduction='sum')
    dist2=Normal(mean_d,sd_d)
    prob=dist2.log_prob(x)   
    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())
    return -1*torch.sum(prob) + KLD, torch.sum(prob), KLD