# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UVDuZYRHyGI7vpS_xuGvZ8gfgW_lj67A
"""

'''
#if you want to work in collab you should uncomment this section
!pip install import_ipynb
!pip install ipynb
import import_ipynb
import ipynb
from google.colab import drive
drive.mount('/content/drive/')
PATH_TRAIN='/content/drive/MyDrive/sad2_project_data/SAD2022Z_Project1_GEX_train.h5ad'
PATH_TEST='/content/drive/MyDrive/sad2_project_data/SAD2022Z_Project1_GEX_test.h5ad'
'''
import argparse
from pathlib import Path

parser = argparse.ArgumentParser()

parser.add_argument("path_train")
parser.add_argument("path_test")

args= parser.parse_args()

PATH_TRAIN = Path(args.path_train)
PATH_TEST = Path(args.path_test)

if not PATH_TRAIN.exists():
    print("The test directory doesn't exist")
    raise SystemExit(1)

if not PATH_TEST.exists():
    print("The train directory doesn't exist")
    raise SystemExit(1)


import torch
from torch.utils.data import DataLoader
from torch.optim import Adam
import os

#your models will be saved in models folder in your working directory
if not os.path.exists("models"):
    # if the demo_folder directory is not present 
    # then create it.
    os.mkdir("models")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks
from autoencoder import Encoder, Decoder, VAE, loss_function
# %cd /content/drive/MyDrive/sad_project1
from data_loading import get_data, normalize

def train(dataloader, model):
    model.train()
    overall_loss = 0
    rec_loss = 0 
    KLD_loss = 0 
    for batch_idx, x in enumerate(dataloader):
        optimizer.zero_grad()

        x_hat, mean, log_var, mean_d, sd_d= model(x)
        loss, rec, KLD= loss_function(x, x_hat,mean,log_var,mean_d, sd_d)
        
        overall_loss += loss.item()
        rec_loss += rec.item()
        KLD_loss += KLD.item()
        
        loss.backward()
        optimizer.step()
    return overall_loss/(batch_idx*batch_size), rec_loss/(batch_idx*batch_size), KLD_loss/(batch_idx*batch_size)

def test(dataloader, model):
  model.eval()
  overall_loss = 0
  rec_loss = 0 
  KLD_loss = 0

  for batch_idx, x in enumerate(dataloader):
    x_hat, mean, log_var, mean_d, sd_d= model(x)
    loss, rec, KLD= loss_function(x, x_hat,mean,log_var,mean_d, sd_d)
    
    overall_loss += loss.item()    
    rec_loss += rec.item()
    KLD_loss += KLD.item()
  
  return overall_loss/(batch_idx*batch_size), rec_loss/(batch_idx*batch_size), KLD_loss/(batch_idx*batch_size)

def training_process(model,epochs,train_loader,test_loader):
  loss_val_array=[]
  loss_array=[]
  reclos_val_array=[]
  reclos_array=[]
  kldlos_val_array=[]
  kldlos_array=[]
  epochs_array=[]

  for epoch in range(epochs):
      
      elbo_train, rec_train, kld_train = train(train_loader,model)
      elbo_test, rec_test, kld_test = test(test_loader, model)
      epochs_array.append(epoch+1)

      loss_array.append(elbo_train)  
      loss_val_array.append(elbo_test)  
      reclos_val_array.append(rec_test)
      reclos_array.append(rec_train)
      kldlos_val_array.append(kld_test)
      kldlos_array.append(kld_train)
      print(f"Epoch: {epoch+1} /",epochs,'...')

      #print(f"Train Loss: {elbo_train:.4f}")
      #print(f"Val Loss: {elbo_test:.4f}")
  return loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array

adata_train, adata_test, data, data_X,data_test, data_test_X = get_data(PATH_TRAIN, PATH_TEST)

batch_size=64
full_dataset=normalize(data)
test_dataset=normalize(data_test)
train_loader=DataLoader(full_dataset,batch_size=batch_size,shuffle=False)
test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)

x_dim = list(full_dataset.shape)[1] 
hidden_dim = 400
latent_dim = 100
lr = 1e-5

epochs = 15


encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=lr)

loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array = training_process(model,epochs,train_loader, test_loader)

PATH='models/model_normal100.pt'
torch.save({'epoch': epochs_array,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': loss_array,
              'loss_val': loss_val_array,
              'reclos_array': reclos_array,
              'reclos_val_array': reclos_val_array,
              'kldlos_array': kldlos_array,
              'kldlos_val_array': kldlos_val_array}, 
              PATH)

print('1/6 models trained')

latent_dim = 50
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=lr)


loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array= training_process(model,epochs,train_loader, test_loader)
PATH='models/model_normal50.pt'
torch.save({'epoch': epochs_array,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': loss_array,
              'loss_val': loss_val_array,
              'reclos_array': reclos_array,
              'reclos_val_array': reclos_val_array,
              'kldlos_array': kldlos_array,
              'kldlos_val_array': kldlos_val_array}, 
              PATH)

print('2/6 models trained')
latent_dim = 300
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=lr)

loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array = training_process(model,epochs,train_loader, test_loader)
PATH='models/model_normal300.pt'
torch.save({'epoch': epochs_array,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': loss_array,
              'loss_val': loss_val_array,
              'reclos_array': reclos_array,
              'reclos_val_array': reclos_val_array,
              'kldlos_array': kldlos_array,
              'kldlos_val_array': kldlos_val_array}, 
              PATH)
print('3/6 models trained')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks
from vae_newdecoder import Encoder,Decoder,VAE, loss_function

batch_size=64
x_dim = list(full_dataset.shape)[1] 
hidden_dim = 400
latent_dim = 100
lr = 1e-5

epochs = 15

full_dataset=torch.tensor(data.values,dtype=torch.float32)
TEST_dataset=torch.tensor(data_test.values,dtype=torch.float32)
train_loader=DataLoader(full_dataset,batch_size=batch_size,shuffle=False)
test_loader=DataLoader(TEST_dataset,batch_size=batch_size,shuffle=False)

latent_dim=100
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=1e-4)


loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array = training_process(model,epochs,train_loader, test_loader)
PATH='models/model_exp100.pt'
torch.save({'epoch': epochs_array,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': loss_array,
              'loss_val': loss_val_array,
              'reclos_array': reclos_array,
              'reclos_val_array': reclos_val_array,
              'kldlos_array': kldlos_array,
              'kldlos_val_array': kldlos_val_array}, 
              PATH)

print('4/6 models trained')

latent_dim=50
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=1e-4)


loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array = training_process(model,epochs,train_loader, test_loader)
PATH='models/model_exp50.pt'
torch.save({'epoch': epochs_array,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': loss_array,
              'loss_val': loss_val_array,
              'reclos_array': reclos_array,
              'reclos_val_array': reclos_val_array,
              'kldlos_array': kldlos_array,
              'kldlos_val_array': kldlos_val_array}, 
              PATH)

print('5/6 models trained')

latent_dim=300
encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)
decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)

model = VAE(Encoder=encoder, Decoder=decoder)
optimizer = Adam(model.parameters(), lr=1e-4)


loss_array, loss_val_array,reclos_array,reclos_val_array,kldlos_array,kldlos_val_array,epochs_array = training_process(model,epochs,train_loader, test_loader)
PATH='models/model_exp300.pt'
torch.save({'epoch': epochs_array,
              'model_state_dict': model.state_dict(),
              'optimizer_state_dict': optimizer.state_dict(),
              'loss': loss_array,
              'loss_val': loss_val_array,
              'reclos_array': reclos_array,
              'reclos_val_array': reclos_val_array,
              'kldlos_array': kldlos_array,
              'kldlos_val_array': kldlos_val_array}, 
              PATH)

print('6/6 models trained')
print('Whole training process completed!!')