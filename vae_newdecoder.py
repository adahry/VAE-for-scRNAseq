# -*- coding: utf-8 -*-
"""VAE_newDECODER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K1Gn8Ra2g8cQ1xROx-Y3hJrH6DKUDQfQ
"""

import torch
import torch.nn as nn
from torch.distributions import Normal,Beta,poisson,Dirichlet,Exponential

class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim):
      super(Encoder, self).__init__()

      self.FC_input = nn.Linear(input_dim, hidden_dim)
      self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)
      self.FC_mean  = nn.Linear(hidden_dim, latent_dim)
      self.FC_var   = nn.Linear (hidden_dim, latent_dim)
      

      self.ReLU = nn.ReLU()
      
      self.training = True

   
  def forward(self, x):
      h = self.ReLU(self.FC_input(x))
      h = self.ReLU(self.FC_input2(h))
      mean = self.FC_mean(h)
      log_var = self.ReLU(self.FC_var(h))  
      sd = torch.exp(0.5*log_var)   
      eps=torch.randn_like(sd) #samplowanie z rozkladu normalnego 
      z=mean+eps*sd         
      return z, mean, log_var


class Decoder(nn.Module):
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super(Decoder, self).__init__()
        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)
        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)

        self.FC_alpha  = nn.Linear(hidden_dim, output_dim)
        self.FC_beta  = nn.Linear(hidden_dim, output_dim)
        
        self.ReLU = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        h     = self.ReLU(self.FC_hidden(x))
        h     = self.ReLU(self.FC_hidden2(h))
        #in dis distribution beta is useless, can be used for gamma
        beta =  torch.tensor([0.5])
        alpha = self.ReLU(self.FC_alpha(h))
        sigma= 0.000001
        alpha = alpha + sigma
        dist= Exponential(alpha)
        sample = dist.sample()
        x_hat=(sample)

        return x_hat,alpha,beta

class VAE(nn.Module):
    def __init__(self, Encoder, Decoder):
        super(VAE, self).__init__()
        self.Encoder = Encoder
        self.Decoder = Decoder
                       
    def forward(self, x):
        z, mean, log_var = self.Encoder(x)
        x_hat,alpha, beta = self.Decoder(z)
        return x_hat, mean, log_var, alpha, beta


def loss_function(x, x_hat,mean,log_var,alpha, beta):
    #MSE = F.mse_loss(x_hat, x)
    dist2=Exponential(alpha)
    prob=dist2.log_prob(x)
    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())
    return -1*torch.sum(prob) +KLD, torch.sum(prob), KLD